{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed03SC1Jm9Yy"
      },
      "source": [
        "# Problemas\n",
        "\n",
        "Como vimos acima, há muitos passos na criação e definição de uma nova rede neural.\n",
        "A grande parte desses ajustes dependem diretamente do problemas.\n",
        "\n",
        "Abaixo, listamos alguns problemas. Todos os problemas e datasets usados vem do [Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/datasets.php).\n",
        "\n",
        "\n",
        "**Seu objetivo é determinar e implementar um modelo para cada problema.**\n",
        "\n",
        "Isso inclui:\n",
        "\n",
        "1. definir uma arquitetura.\n",
        "Por enquanto usando somente camadas [Lineares](https://pytorch.org/docs/stable/nn.html#linear), porém podemos variar as ativações, como [Sigmoid](https://pytorch.org/docs/stable/nn.html#sigmoid), [Tanh](https://pytorch.org/docs/stable/nn.html#tanh), [ReLU](https://pytorch.org/docs/stable/nn.html#relu), [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html), [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html), [SeLU](https://pytorch.org/docs/stable/generated/torch.nn.SELU.html), [PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html), [RReLU](https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html)\n",
        "2. definir uma função de custo. Algums opções que vimos previamente incluem[L1](https://pytorch.org/docs/stable/nn.html#l1loss), [L2/MSE](https://pytorch.org/docs/stable/nn.html#mseloss), [Huber/SmoothL1](https://pytorch.org/docs/stable/nn.html#smoothl1loss), [*Cross-Entropy*](https://pytorch.org/docs/stable/nn.html#crossentropyloss), [Hinge](https://pytorch.org/docs/stable/nn.html#hingeembeddingloss)), e\n",
        "3. definir um algoritmo de otimização ([SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), [RMSProp](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop), [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam))\n",
        "\n",
        "A leitura do dado assim como a função de treinamento já estão implementados para você."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bzMRy-nFKua"
      },
      "source": [
        "# Preâmbulo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW-VATPAldgt"
      },
      "source": [
        "# imports basicos\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5RRCEpFKug",
        "outputId": "1dd452ed-e04e-4ed7-c743-85731ed12fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7d640a851dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNofnRSOFKul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48f4ffc-e57a-462c-e5c5-cca4212d7603"
      },
      "source": [
        "# Test if GPU is avaliable, if not, use cpu instead\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids = list(range(n))\n",
        "device"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções básicas\n",
        "\n",
        "Use a função `load_array ` declarada a seguir se voce ja tem os dados armazenados em um **array** (por exemplo um array do numpy, o `np.array`). Pode acontecer de que os nossos dados vêm simplesmente de um dataset que pode ser armazennado em um array, e portanto não é necessário fazer os outros passos mais complicados como carregar os dados do disco, etc; basta que possamos recuperar esses dados em *batches* aleatórios. O resultado dessa função é um `DataLoader` do Pytorch com os dados que fornecemos de entrada, e que permite que os acessamos da seguinte forma:\n",
        "\n",
        "```python\n",
        "data_loader = load_array(X, y, batch_size=32, is_train=True)\n",
        "for x_batch, y_batch in data_loader:\n",
        "    ### ... nossa iteração de treinamento aqui.\n",
        "```\n",
        "\n",
        "Essa função recebe como parâmetro os seguintes valores:\n",
        "\n",
        "- `features`: um array que contém as features de todas as instâncias do dataset. Por exemplo, no caso do MNIST seria um array de tamanho `(60000, 28, 28, 1)` com todas as imagens do dataset de treino.\n",
        "- `labels`: um array que contém os rótulos de cada instância de dados. No caso do MNIST, seria um array de tamanho `(60000,)` em que a posição `i` contém o rótulo do dígito da posição `i` do array `features`.\n",
        "- `batch_size`: tamanho do batch desejado\n",
        "- `is_train`: um booleano que indica se o dataset que estamos criando é o conjunto de treinamento ou não (conjunto de teste). A única mudança que isso causa no `Dataloader` resultante é que se for o conjunto de treinamento ele cria batches aleatórios.\n"
      ],
      "metadata": {
        "id": "vdHX3JaM_-7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    \"\"\"Construct a Torch data loader\"\"\"\n",
        "\n",
        "    ## transform the input arrays in a tensor in case they are not\n",
        "    if type(features) != torch.tensor:\n",
        "        features = torch.tensor(features)\n",
        "    if type(labels) != torch.tensor:\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    ## create a Pytorch Dataset and DataLoader with the input data\n",
        "    dataset = torch.utils.data.TensorDataset(features, labels)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)"
      ],
      "metadata": {
        "id": "xAN7JCEPAEU4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a função `evaluate_accuracy` para calcular a acurácia e a *loss-function* para a rede em um conjunto de dados. Note que essa função pode ser usada tanto para avaliar a rede no conjunto de teste (no caso que usamos o `DataLoader` de teste) quanto o conjunto de treinamento (se usamos o `DataLoader` de treinamento). Os parâmetros são:\n",
        "- `data_iter`: um `DataLoader` que contém os dados que queremos usar para avaliar a rede. Repare que esse parâmetro tipicamente é o ojeto que obtemos como saída da função `load_array` para montar o nosso `DataLoader`.\n",
        "- `net`: a rede que queremos avaliar\n",
        "- `loss`: a nossa *loss-function*. Pode ser um objeto de qualquer uma das funções de perda que mencionamos acima no começo do notebook.\n",
        "\n",
        "O resultado dessa função é uma tupla em que o primeiro valor é a acurácia e o segundo a função de custo calculados."
      ],
      "metadata": {
        "id": "0Qn0Sh7KBLE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "\n",
        "    ## valores \"acumuladores\", que guardam a soma de, respectivamente, quantas instâncias\n",
        "    ## prevemos corretamente, quantas instâncias percorremos no dataset, e o valor da loss; para\n",
        "    ## todos os batches\n",
        "    acc_sum, n, l = 0, 0, 0\n",
        "\n",
        "    ## muda a rede para o \"modo de teste\". O que isso faz é mudar o comportamento de alguns módulos da rede,\n",
        "    ## como os módulos de Dropout e BatchNorm, que funcionam de forma diferente quando estamos treinando ou\n",
        "    ## quando estamos avaliando (ou usando em produção) a rede\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          y_hat = net(X)\n",
        "          l += loss(y_hat, y.long())\n",
        "\n",
        "          ## aqui estamos calculando a quantidade de previsões que temos correta para o batch atual. o resultado\n",
        "          ## do argmax é a posição de `y_hat` que possui o maior valor. Consequentemente isso resulta na classe que\n",
        "          ## a rede deu o maior score.\n",
        "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "\n",
        "          ##\n",
        "          n += y.size(0)\n",
        "\n",
        "    return acc_sum / n, l.item() / len(data_iter)"
      ],
      "metadata": {
        "id": "SRGaUQsEFH0g"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função `train_validate` é a função que implementa nossas iterações de treinamento padrão. Ela ja faz o trabalho de percorrer o dataset inteiro para cada época, e também de tempos em tempos avaliar a rede e mostar os resultados na tela. Para isso ela faz chamadas à função `evaluate_accuracy` declarada anteriormente (entre outras coisas). Essa função tem os segugintes parâmetros:\n",
        "- `net`: a rede que queremos treinar\n",
        "- `train_iter` e `test_iter`: nossos `DataLoaders` que criamos para acessar os dados. Esses DataLoaders podem ser criados com a função `load_array` declarada acima.\n",
        "- `trainer`: é o nosso otimizador. Podemos usar aqui qualquer um dos otimizadores que escolhermos da lista citada no começo desse notebook.\n",
        "- `loss`: a loss function que escolhemos para otimizar. Pode ser qualquer um das funções de custo citadas no começo do notebook.\n",
        "- `num_epochs`: a quantidade de épocas pelas quais queremos que o treinamento ocorra.\n",
        "- `type`: o tipo de tarefa que estamos lidando. Se for um problema de regressão, usamos `type='regression'`, e se for um problema de classificação, usamos `type='classification'`. Esse parâmetro é necessário para a função, por exemplo, saber quais métricas ele vai mostrar (acurácia, ou apenas o MSE, etc.)"
      ],
      "metadata": {
        "id": "mBmoehobG-TC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oSVf8u1Oi1m"
      },
      "source": [
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, trainer, loss, num_epochs, type='regression'):\n",
        "    print('training on', device)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            if type == 'regression':\n",
        "              l = loss(y_hat, y.float())\n",
        "            else:\n",
        "              l = loss(y_hat, y.long())\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.size(0)\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
        "        if type == 'regression':\n",
        "          print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n",
        "                % (epoch + 1, train_l_sum / len(train_iter), test_loss, time.time() - start))\n",
        "        else:\n",
        "          print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
        "              'test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n",
        "                 test_acc, time.time() - start))\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a função a seguir para inicializar os pesos da rede. Ela recebe como parâmetro um módulo da rede neural, e se for uma camada linear ele inicializa os pesos e os bias dessa camada. Embora possa parecer complicado de precisar chamar essa função para todas as camadas lineares da nossa rede, o módulos do Pytorch (que incluem tanto as redes criadas com o `nn.Sequential` ou com `nn.Module`) possuem a função `net.apply()` que recebe como parâmetro uma função e aplica ela a todos os submódulos da rede. Portanto, depois de ter criado a nossa rede, podemos chamar:\n",
        "\n",
        "```python\n",
        "net.apply(weights_init)\n",
        "```\n",
        "que automaticamente todas as camadas `nn.Linear` serão inicializadas. Caso queira saber mais sobre o `.apply()`, veja o seguinte [link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply)."
      ],
      "metadata": {
        "id": "ExWzvYS9JqTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para inicializar pesos da rede\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal\n",
        "        m.bias.data.fill_(0)"
      ],
      "metadata": {
        "id": "mkeIXH1PJqpA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0m-qic-0Wnl"
      },
      "source": [
        "# Problema 1\n",
        "\n",
        "Neste problema, você receberá 14 *features* coletadas de pacientes e tentará predizer se eles tem algum sinal de doença cardíaca. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aqui fazemos o download do dataset usando o `!wget`. Se estamos rodando em um servidor linux (como é o caso do Colab),\n",
        "## podemos usar comandos do linux precedidos pelo \"!\". Por exemplo podemos fazer !ls para listar os arquivos da instância do colab.\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
        "\n",
        "## aqui fazemos um tratamento inicial dos dados. \"np.genfromtxt\" lê os dados de um arquivo .txt e transforma em\n",
        "## um array. Pode ser interessante abrir o arquivo para verificar como os dados chegaram. Se estiver no colab, voce\n",
        "## pode verificar o arquivo \"processed.cleveland.data\" clicando na pastinha do canto esquerdo da página. a função\n",
        "## \"np.nan_to_num\" trata valores NaN e infinitos no dataset.\n",
        "data = np.genfromtxt('processed.cleveland.data', delimiter=',', dtype=np.float32)\n",
        "data = np.nan_to_num(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2V7qc5FNZpC",
        "outputId": "a333e7ae-8f3f-46c4-dec4-fcb0c0880376"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-19 11:05:25--  https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘processed.cleveland.data.1’\n",
            "\n",
            "\r          processed     [<=>                 ]       0  --.-KB/s               \rprocessed.cleveland     [ <=>                ]  18.03K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-05-19 11:05:25 (2.18 MB/s) - ‘processed.cleveland.data.1’ saved [18461]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aqui separamos os dados entre features (X) e rótulo (y), e depois separamos em um conjunto de treinamento e teste\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdIA4XYoNa_2",
        "outputId": "4803f7ed-a580-4350-f1d1-d228187c7313"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(303, 14) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6.    0. ]\n",
            "(303, 13) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6. ]\n",
            "(303,) 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUYOPZYH0Ztc",
        "outputId": "a4b04f77-866d-4f90-b5b7-7f52216c04ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "## aqui criamos nossos DataLoaders para conseguirmos iterar nos dados\n",
        "batch_size = 64\n",
        "train_iter = load_array(X_train, Y_train, batch_size)\n",
        "test_iter = load_array(X_test, Y_test, batch_size, False)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataLoader' object has no attribute 'labels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-d3dc912f2345>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "178XNdRUpiQW"
      },
      "source": [
        "class NET (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NET, self).__init__()\n",
        "\n",
        "    self.nn = nn.Sequential(\n",
        "\n",
        "      nn.Linear(14, 12),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(12, 10),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(10, 8),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(8, 5),\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = self.nn(X)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NET().to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "r1rrFZbEKrXp",
        "outputId": "25db3699-fd74-416a-88fe-4648ffeb1fb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NET' object has no attribute 'float32'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-69c316858424>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NET' object has no attribute 'float32'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_validate(net = model, train_iter = train_iter , test_iter = test_iter, trainer = opt, loss = loss, num_epochs = 20, type='classification')"
      ],
      "metadata": {
        "id": "DUXmFyP4Lslo",
        "outputId": "95e983c0-8246-4b75-a6a5-47baaf54efb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f91095c670bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'classification'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-7ef789fac9c9>\u001b[0m in \u001b[0;36mtrain_validate\u001b[0;34m(net, train_iter, test_iter, trainer, loss, num_epochs, type)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     new_grads.append(\n\u001b[0;32m--> 220\u001b[0;31m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     )\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDaRVNq1aMpm"
      },
      "source": [
        "# Problema 2\n",
        "\n",
        "Neste problema, você receberá 90 *features* extraídas de diversas músicas (datadas de 1922 até 2011) e deve predizer o ano de cada música. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWdBT3zhW_Y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea82128a-aa9a-49ce-abc9-3a4568ae4232"
      },
      "source": [
        "# download do dataset\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
        "!unzip YearPredictionMSD.txt.zip\n",
        "data = np.genfromtxt('YearPredictionMSD.txt', delimiter=',', dtype=np.float32)\n",
        "\n",
        "print(data[0, :])\n",
        "X, y = data[:, 1:], data[:, 0]\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 02:51:16--  http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211011981 (201M) [application/x-httpd-php]\n",
            "Saving to: ‘YearPredictionMSD.txt.zip’\n",
            "\n",
            "YearPredictionMSD.t 100%[===================>] 201.24M  54.2MB/s    in 4.1s    \n",
            "\n",
            "2022-03-17 02:51:20 (49.3 MB/s) - ‘YearPredictionMSD.txt.zip’ saved [211011981/211011981]\n",
            "\n",
            "Archive:  YearPredictionMSD.txt.zip\n",
            "  inflating: YearPredictionMSD.txt   \n",
            "[ 2.0010000e+03  4.9943569e+01  2.1471140e+01  7.3077499e+01\n",
            "  8.7486095e+00 -1.7406281e+01 -1.3099050e+01 -2.5012020e+01\n",
            " -1.2232570e+01  7.8308902e+00 -2.4678299e+00  3.3213601e+00\n",
            " -2.3152101e+00  1.0205560e+01  6.1110913e+02  9.5108960e+02\n",
            "  6.9811426e+02  4.0898486e+02  3.8370911e+02  3.2651511e+02\n",
            "  2.3811327e+02  2.5142413e+02  1.8717351e+02  1.0042652e+02\n",
            "  1.7919498e+02 -8.4155798e+00 -3.1787039e+02  9.5862663e+01\n",
            "  4.8102589e+01 -9.5663033e+01 -1.8062149e+01  1.9698400e+00\n",
            "  3.4424381e+01  1.1726700e+01  1.3679000e+00  7.7944398e+00\n",
            " -3.6994001e-01 -1.3367851e+02 -8.3261650e+01 -3.7297649e+01\n",
            "  7.3046669e+01 -3.7366840e+01 -3.1385300e+00 -2.4215309e+01\n",
            " -1.3230660e+01  1.5938090e+01 -1.8604780e+01  8.2154793e+01\n",
            "  2.4057980e+02 -1.0294070e+01  3.1584311e+01 -2.5381870e+01\n",
            " -3.9077201e+00  1.3292580e+01  4.1550598e+01 -7.2627201e+00\n",
            " -2.1008631e+01  1.0550848e+02  6.4298561e+01  2.6084810e+01\n",
            " -4.4591099e+01 -8.3065701e+00  7.9370599e+00 -1.0736600e+01\n",
            " -9.5447662e+01 -8.2033073e+01 -3.5591942e+01  4.6952500e+00\n",
            "  7.0956261e+01  2.8091391e+01  6.0201502e+00 -3.7137669e+01\n",
            " -4.1124500e+01 -8.4081602e+00  7.1987700e+00 -8.6017599e+00\n",
            " -5.9085698e+00 -1.2324370e+01  1.4687340e+01 -5.4321251e+01\n",
            "  4.0147861e+01  1.3016200e+01 -5.4405479e+01  5.8993671e+01\n",
            "  1.5373440e+01  1.1114399e+00 -2.3087931e+01  6.8407951e+01\n",
            " -1.8222300e+00 -2.7463480e+01  2.2632699e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoHCfAjzft_3"
      },
      "source": [
        "# IMPLEMENTE AQUI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNMAyyX8jSb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74506ae1-a869-4351-c83c-57c4a8bce755"
      },
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\n",
        "y = net(torch.Tensor(test_features[0:5, :]).to(device))\n",
        "print(y, test_labels[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2061.2070],\n",
            "        [2095.2788],\n",
            "        [2045.4172],\n",
            "        [1757.2596],\n",
            "        [1941.1250]], device='cuda:0', grad_fn=<AddmmBackward0>) [2008. 2001. 2006. 2008. 1998.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8hG7HCDUib"
      },
      "source": [
        "# Problema 3\n",
        "\n",
        "Neste problema, você receberá várias *features* (como altura média, inclinação, etc) descrevendo uma região e o modelo deve predizer qual o tipo da região (floresta, montanha, etc). Mais informações sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/covertype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZcIXGqBDznB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33b284b-d5e7-416d-cb51-0541b559329a"
      },
      "source": [
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
        "!gzip covtype.data.gz\n",
        "data = np.genfromtxt('covtype.data', delimiter=',', dtype=np.float32)\n",
        "\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "train_labels = train_labels - 1\n",
        "test_labels = test_labels - 1\n",
        "\n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 03:14:12--  http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11240707 (11M) [application/x-httpd-php]\n",
            "Saving to: ‘covtype.data.gz’\n",
            "\n",
            "covtype.data.gz     100%[===================>]  10.72M  20.0MB/s    in 0.5s    \n",
            "\n",
            "2022-03-17 03:14:13 (20.0 MB/s) - ‘covtype.data.gz’ saved [11240707/11240707]\n",
            "\n",
            "gzip: covtype.data.gz already has .gz suffix -- unchanged\n",
            "(581012, 55) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00]\n",
            "(581012, 54) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            "(581012,) 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O0HJVIOZZW4"
      },
      "source": [
        "# IMPLEMENTE AQUI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOAxsRz0rpZl"
      },
      "source": [
        "# Problema 4\n",
        "\n",
        "Neste problema, você receberá 11 *features* extraídas de tipos de vinhos, e terá que predizer um *score* para cada vinho. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTzA0O6rusi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e11b5c-b4aa-4b94-f699-c6d07bd60361"
      },
      "source": [
        "# download do dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
        "data_red = np.genfromtxt('winequality-red.csv', delimiter=';', dtype=np.float32, skip_header=1)\n",
        "data_white = np.genfromtxt('winequality-white.csv', delimiter=';', dtype=np.float32, skip_header=1)\n",
        "data = np.concatenate((data_red, data_white), axis=0)\n",
        "data = np.nan_to_num(data)\n",
        "\n",
        "print(data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, y.shape)\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 03:35:50--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84199 (82K) [application/x-httpd-php]\n",
            "Saving to: ‘winequality-red.csv’\n",
            "\n",
            "winequality-red.csv 100%[===================>]  82.23K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-03-17 03:35:51 (780 KB/s) - ‘winequality-red.csv’ saved [84199/84199]\n",
            "\n",
            "--2022-03-17 03:35:51--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264426 (258K) [application/x-httpd-php]\n",
            "Saving to: ‘winequality-white.csv’\n",
            "\n",
            "winequality-white.c 100%[===================>] 258.23K  1.24MB/s    in 0.2s    \n",
            "\n",
            "2022-03-17 03:35:51 (1.24 MB/s) - ‘winequality-white.csv’ saved [264426/264426]\n",
            "\n",
            "[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
            "  0.56    9.4     5.    ]\n",
            "(6497, 11) (6497,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E7pwns4rx9l"
      },
      "source": [
        "# IMPLEMENTE AQUI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-At99Iqzryg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83436484-3c68-4973-dde8-191f9453824a"
      },
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\n",
        "y = net(torch.Tensor(test_features[0:5, :]).to(device))\n",
        "print(y, test_labels[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6.3718],\n",
            "        [5.6745],\n",
            "        [6.1786],\n",
            "        [5.6269],\n",
            "        [5.9695]], device='cuda:0', grad_fn=<AddmmBackward0>) [8. 5. 7. 6. 6.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LVIg2E-W7hVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}