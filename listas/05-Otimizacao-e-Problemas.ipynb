{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed03SC1Jm9Yy"
      },
      "source": [
        "# Problemas\n",
        "\n",
        "Como vimos acima, há muitos passos na criação e definição de uma nova rede neural.\n",
        "A grande parte desses ajustes dependem diretamente do problemas.\n",
        "\n",
        "Abaixo, listamos alguns problemas. Todos os problemas e datasets usados vem do [Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/datasets.php).\n",
        "\n",
        "\n",
        "**Seu objetivo é determinar e implementar um modelo para cada problema.**\n",
        "\n",
        "Isso inclui:\n",
        "\n",
        "1. definir uma arquitetura.\n",
        "Por enquanto usando somente camadas [Lineares](https://pytorch.org/docs/stable/nn.html#linear), porém podemos variar as ativações, como [Sigmoid](https://pytorch.org/docs/stable/nn.html#sigmoid), [Tanh](https://pytorch.org/docs/stable/nn.html#tanh), [ReLU](https://pytorch.org/docs/stable/nn.html#relu), [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html), [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html), [SeLU](https://pytorch.org/docs/stable/generated/torch.nn.SELU.html), [PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html), [RReLU](https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html)\n",
        "2. definir uma função de custo. Algums opções que vimos previamente incluem[L1](https://pytorch.org/docs/stable/nn.html#l1loss), [L2/MSE](https://pytorch.org/docs/stable/nn.html#mseloss), [Huber/SmoothL1](https://pytorch.org/docs/stable/nn.html#smoothl1loss), [*Cross-Entropy*](https://pytorch.org/docs/stable/nn.html#crossentropyloss), [Hinge](https://pytorch.org/docs/stable/nn.html#hingeembeddingloss)), e\n",
        "3. definir um algoritmo de otimização ([SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), [RMSProp](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop), [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam))\n",
        "\n",
        "A leitura do dado assim como a função de treinamento já estão implementados para você."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bzMRy-nFKua"
      },
      "source": [
        "# Preâmbulo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW-VATPAldgt"
      },
      "source": [
        "# imports basicos\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5RRCEpFKug",
        "outputId": "7c16b26c-8b2e-465b-fe33-ef708215fce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7829e4178c10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNofnRSOFKul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ddc69d7-7521-470f-cd31-1b0d23362e43"
      },
      "source": [
        "# Test if GPU is avaliable, if not, use cpu instead\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids = list(range(n))\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções básicas\n",
        "\n",
        "Use a função `load_array ` declarada a seguir se voce ja tem os dados armazenados em um **array** (por exemplo um array do numpy, o `np.array`). Pode acontecer de que os nossos dados vêm simplesmente de um dataset que pode ser armazennado em um array, e portanto não é necessário fazer os outros passos mais complicados como carregar os dados do disco, etc; basta que possamos recuperar esses dados em *batches* aleatórios. O resultado dessa função é um `DataLoader` do Pytorch com os dados que fornecemos de entrada, e que permite que os acessamos da seguinte forma:\n",
        "\n",
        "```python\n",
        "data_loader = load_array(X, y, batch_size=32, is_train=True)\n",
        "for x_batch, y_batch in data_loader:\n",
        "    ### ... nossa iteração de treinamento aqui.\n",
        "```\n",
        "\n",
        "Essa função recebe como parâmetro os seguintes valores:\n",
        "\n",
        "- `features`: um array que contém as features de todas as instâncias do dataset. Por exemplo, no caso do MNIST seria um array de tamanho `(60000, 28, 28, 1)` com todas as imagens do dataset de treino.\n",
        "- `labels`: um array que contém os rótulos de cada instância de dados. No caso do MNIST, seria um array de tamanho `(60000,)` em que a posição `i` contém o rótulo do dígito da posição `i` do array `features`.\n",
        "- `batch_size`: tamanho do batch desejado\n",
        "- `is_train`: um booleano que indica se o dataset que estamos criando é o conjunto de treinamento ou não (conjunto de teste). A única mudança que isso causa no `Dataloader` resultante é que se for o conjunto de treinamento ele cria batches aleatórios.\n"
      ],
      "metadata": {
        "id": "vdHX3JaM_-7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    \"\"\"Construct a Torch data loader\"\"\"\n",
        "\n",
        "    ## transform the input arrays in a tensor in case they are not\n",
        "    if type(features) != torch.tensor:\n",
        "        features = torch.tensor(features)\n",
        "    if type(labels) != torch.tensor:\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    ## create a Pytorch Dataset and DataLoader with the input data\n",
        "    dataset = torch.utils.data.TensorDataset(features, labels)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)"
      ],
      "metadata": {
        "id": "xAN7JCEPAEU4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a função `evaluate_accuracy` para calcular a acurácia e a *loss-function* para a rede em um conjunto de dados. Note que essa função pode ser usada tanto para avaliar a rede no conjunto de teste (no caso que usamos o `DataLoader` de teste) quanto o conjunto de treinamento (se usamos o `DataLoader` de treinamento). Os parâmetros são:\n",
        "- `data_iter`: um `DataLoader` que contém os dados que queremos usar para avaliar a rede. Repare que esse parâmetro tipicamente é o ojeto que obtemos como saída da função `load_array` para montar o nosso `DataLoader`.\n",
        "- `net`: a rede que queremos avaliar\n",
        "- `loss`: a nossa *loss-function*. Pode ser um objeto de qualquer uma das funções de perda que mencionamos acima no começo do notebook.\n",
        "\n",
        "O resultado dessa função é uma tupla em que o primeiro valor é a acurácia e o segundo a função de custo calculados."
      ],
      "metadata": {
        "id": "0Qn0Sh7KBLE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "\n",
        "    ## valores \"acumuladores\", que guardam a soma de, respectivamente, quantas instâncias\n",
        "    ## prevemos corretamente, quantas instâncias percorremos no dataset, e o valor da loss; para\n",
        "    ## todos os batches\n",
        "    acc_sum, n, l = 0, 0, 0\n",
        "\n",
        "    ## muda a rede para o \"modo de teste\". O que isso faz é mudar o comportamento de alguns módulos da rede,\n",
        "    ## como os módulos de Dropout e BatchNorm, que funcionam de forma diferente quando estamos treinando ou\n",
        "    ## quando estamos avaliando (ou usando em produção) a rede\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          y_hat = net(X)\n",
        "          l += loss(y_hat, y.long())\n",
        "\n",
        "          ## aqui estamos calculando a quantidade de previsões que temos correta para o batch atual. o resultado\n",
        "          ## do argmax é a posição de `y_hat` que possui o maior valor. Consequentemente isso resulta na classe que\n",
        "          ## a rede deu o maior score.\n",
        "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "\n",
        "          ##\n",
        "          n += y.size(0)\n",
        "\n",
        "    return acc_sum / n, l.item() / len(data_iter)"
      ],
      "metadata": {
        "id": "SRGaUQsEFH0g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função `train_validate` é a função que implementa nossas iterações de treinamento padrão. Ela ja faz o trabalho de percorrer o dataset inteiro para cada época, e também de tempos em tempos avaliar a rede e mostar os resultados na tela. Para isso ela faz chamadas à função `evaluate_accuracy` declarada anteriormente (entre outras coisas). Essa função tem os segugintes parâmetros:\n",
        "- `net`: a rede que queremos treinar\n",
        "- `train_iter` e `test_iter`: nossos `DataLoaders` que criamos para acessar os dados. Esses DataLoaders podem ser criados com a função `load_array` declarada acima.\n",
        "- `trainer`: é o nosso otimizador. Podemos usar aqui qualquer um dos otimizadores que escolhermos da lista citada no começo desse notebook.\n",
        "- `loss`: a loss function que escolhemos para otimizar. Pode ser qualquer um das funções de custo citadas no começo do notebook.\n",
        "- `num_epochs`: a quantidade de épocas pelas quais queremos que o treinamento ocorra.\n",
        "- `type`: o tipo de tarefa que estamos lidando. Se for um problema de regressão, usamos `type='regression'`, e se for um problema de classificação, usamos `type='classification'`. Esse parâmetro é necessário para a função, por exemplo, saber quais métricas ele vai mostrar (acurácia, ou apenas o MSE, etc.)"
      ],
      "metadata": {
        "id": "mBmoehobG-TC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oSVf8u1Oi1m"
      },
      "source": [
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, trainer, loss, num_epochs, type='regression'):\n",
        "    print('training on', device)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            if type == 'regression':\n",
        "              l = loss(y_hat, y.float())\n",
        "            else:\n",
        "              l = loss(y_hat, y.long())\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.size(0)\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
        "        if type == 'regression':\n",
        "          print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n",
        "                % (epoch + 1, train_l_sum / len(train_iter), test_loss, time.time() - start))\n",
        "        else:\n",
        "          print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
        "              'test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n",
        "                 test_acc, time.time() - start))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a função a seguir para inicializar os pesos da rede. Ela recebe como parâmetro um módulo da rede neural, e se for uma camada linear ele inicializa os pesos e os bias dessa camada. Embora possa parecer complicado de precisar chamar essa função para todas as camadas lineares da nossa rede, o módulos do Pytorch (que incluem tanto as redes criadas com o `nn.Sequential` ou com `nn.Module`) possuem a função `net.apply()` que recebe como parâmetro uma função e aplica ela a todos os submódulos da rede. Portanto, depois de ter criado a nossa rede, podemos chamar:\n",
        "\n",
        "```python\n",
        "net.apply(weights_init)\n",
        "```\n",
        "que automaticamente todas as camadas `nn.Linear` serão inicializadas. Caso queira saber mais sobre o `.apply()`, veja o seguinte [link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply)."
      ],
      "metadata": {
        "id": "ExWzvYS9JqTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para inicializar pesos da rede\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal\n",
        "        m.bias.data.fill_(0)"
      ],
      "metadata": {
        "id": "mkeIXH1PJqpA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0m-qic-0Wnl"
      },
      "source": [
        "# Problema 1\n",
        "\n",
        "Neste problema, você receberá 14 *features* coletadas de pacientes e tentará predizer se eles tem algum sinal de doença cardíaca. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aqui fazemos o download do dataset usando o `!wget`. Se estamos rodando em um servidor linux (como é o caso do Colab),\n",
        "## podemos usar comandos do linux precedidos pelo \"!\". Por exemplo podemos fazer !ls para listar os arquivos da instância do colab.\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
        "\n",
        "## aqui fazemos um tratamento inicial dos dados. \"np.genfromtxt\" lê os dados de um arquivo .txt e transforma em\n",
        "## um array. Pode ser interessante abrir o arquivo para verificar como os dados chegaram. Se estiver no colab, voce\n",
        "## pode verificar o arquivo \"processed.cleveland.data\" clicando na pastinha do canto esquerdo da página. a função\n",
        "## \"np.nan_to_num\" trata valores NaN e infinitos no dataset.\n",
        "data = np.genfromtxt('processed.cleveland.data', delimiter=',', dtype=np.float32)\n",
        "data = np.nan_to_num(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2V7qc5FNZpC",
        "outputId": "7e3f1580-3c5f-40d9-8893-c69688a526a7"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-20 01:38:39--  https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘processed.cleveland.data.1’\n",
            "\n",
            "\r          processed     [<=>                 ]       0  --.-KB/s               \rprocessed.cleveland     [ <=>                ]  18.03K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-20 01:38:39 (693 KB/s) - ‘processed.cleveland.data.1’ saved [18461]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## aqui separamos os dados entre features (X) e rótulo (y), e depois separamos em um conjunto de treinamento e teste\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdIA4XYoNa_2",
        "outputId": "54b47a25-2930-421a-df0f-b5bf17c8d157"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(303, 14) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6.    0. ]\n",
            "(303, 13) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6. ]\n",
            "(303,) 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "IMSuSH47nYMF"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUYOPZYH0Ztc"
      },
      "source": [
        "## aqui criamos nossos DataLoaders para conseguirmos iterar nos dados\n",
        "batch_size = 32\n",
        "train_iter = load_array(X_train, Y_train, batch_size)\n",
        "test_iter = load_array(X_test, Y_test, batch_size, False)\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "178XNdRUpiQW"
      },
      "source": [
        "class NET (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NET, self).__init__()\n",
        "\n",
        "    self.nn = nn.Sequential(\n",
        "\n",
        "      nn.Linear(13, 16),\n",
        "      nn.Dropout(0.5),\n",
        "\n",
        "      nn.Tanh(),\n",
        "\n",
        "\n",
        "      nn.Linear(16, 12),\n",
        "      nn.Dropout(0.3),\n",
        "      nn.Tanh(),\n",
        "\n",
        "      nn.Linear(12, 8),\n",
        "      nn.Dropout(0.15),\n",
        "      nn.Tanh(),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      nn.Linear(8, 5),\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = self.nn(X)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NET().to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(), lr=0.03, weight_decay = 0.0001)"
      ],
      "metadata": {
        "id": "r1rrFZbEKrXp"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_validate(net = model, train_iter = train_iter , test_iter = test_iter, trainer = opt, loss = loss, num_epochs = 100, type='classification')"
      ],
      "metadata": {
        "id": "DUXmFyP4Lslo",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cab8c68-dd0c-4c29-e1fe-50d4909b98b0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 1.4614, train acc 0.420, test loss 1.0424, test acc 0.604, time 0.0 sec\n",
            "epoch 2, train loss 0.9895, train acc 0.637, test loss 1.0422, test acc 0.571, time 0.0 sec\n",
            "epoch 3, train loss 0.9269, train acc 0.637, test loss 1.0340, test acc 0.582, time 0.0 sec\n",
            "epoch 4, train loss 0.8981, train acc 0.646, test loss 1.0679, test acc 0.582, time 0.0 sec\n",
            "epoch 5, train loss 0.8579, train acc 0.656, test loss 1.0505, test acc 0.549, time 0.0 sec\n",
            "epoch 6, train loss 0.8325, train acc 0.632, test loss 1.0706, test acc 0.560, time 0.0 sec\n",
            "epoch 7, train loss 0.7780, train acc 0.665, test loss 1.0893, test acc 0.582, time 0.0 sec\n",
            "epoch 8, train loss 0.7504, train acc 0.708, test loss 1.0958, test acc 0.560, time 0.0 sec\n",
            "epoch 9, train loss 0.7052, train acc 0.675, test loss 1.1457, test acc 0.571, time 0.0 sec\n",
            "epoch 10, train loss 0.6569, train acc 0.712, test loss 1.1770, test acc 0.582, time 0.0 sec\n",
            "epoch 11, train loss 0.6413, train acc 0.722, test loss 1.1900, test acc 0.571, time 0.0 sec\n",
            "epoch 12, train loss 0.5897, train acc 0.736, test loss 1.2275, test acc 0.571, time 0.0 sec\n",
            "epoch 13, train loss 0.5544, train acc 0.774, test loss 1.3116, test acc 0.505, time 0.0 sec\n",
            "epoch 14, train loss 0.5365, train acc 0.778, test loss 1.2929, test acc 0.549, time 0.0 sec\n",
            "epoch 15, train loss 0.4833, train acc 0.783, test loss 1.3817, test acc 0.560, time 0.0 sec\n",
            "epoch 16, train loss 0.4805, train acc 0.802, test loss 1.4569, test acc 0.505, time 0.0 sec\n",
            "epoch 17, train loss 0.5225, train acc 0.816, test loss 1.4380, test acc 0.560, time 0.0 sec\n",
            "epoch 18, train loss 0.4791, train acc 0.778, test loss 1.5037, test acc 0.538, time 0.0 sec\n",
            "epoch 19, train loss 0.4258, train acc 0.821, test loss 1.6286, test acc 0.538, time 0.0 sec\n",
            "epoch 20, train loss 0.3860, train acc 0.844, test loss 1.6594, test acc 0.549, time 0.0 sec\n",
            "epoch 21, train loss 0.3620, train acc 0.863, test loss 1.6806, test acc 0.505, time 0.0 sec\n",
            "epoch 22, train loss 0.3604, train acc 0.868, test loss 1.8078, test acc 0.516, time 0.0 sec\n",
            "epoch 23, train loss 0.2934, train acc 0.887, test loss 1.8938, test acc 0.549, time 0.0 sec\n",
            "epoch 24, train loss 0.2949, train acc 0.873, test loss 1.8991, test acc 0.593, time 0.0 sec\n",
            "epoch 25, train loss 0.2562, train acc 0.906, test loss 2.0239, test acc 0.560, time 0.0 sec\n",
            "epoch 26, train loss 0.2533, train acc 0.901, test loss 1.9622, test acc 0.560, time 0.0 sec\n",
            "epoch 27, train loss 0.2813, train acc 0.882, test loss 1.8762, test acc 0.582, time 0.0 sec\n",
            "epoch 28, train loss 0.2253, train acc 0.925, test loss 2.0316, test acc 0.593, time 0.0 sec\n",
            "epoch 29, train loss 0.2101, train acc 0.920, test loss 2.0659, test acc 0.615, time 0.0 sec\n",
            "epoch 30, train loss 0.1932, train acc 0.934, test loss 2.0477, test acc 0.560, time 0.0 sec\n",
            "epoch 31, train loss 0.1706, train acc 0.948, test loss 2.1621, test acc 0.549, time 0.0 sec\n",
            "epoch 32, train loss 0.1714, train acc 0.943, test loss 2.2149, test acc 0.527, time 0.0 sec\n",
            "epoch 33, train loss 0.1778, train acc 0.934, test loss 2.1377, test acc 0.560, time 0.0 sec\n",
            "epoch 34, train loss 0.1755, train acc 0.939, test loss 2.1863, test acc 0.560, time 0.0 sec\n",
            "epoch 35, train loss 0.1393, train acc 0.948, test loss 2.2396, test acc 0.549, time 0.0 sec\n",
            "epoch 36, train loss 0.1871, train acc 0.948, test loss 2.3296, test acc 0.549, time 0.0 sec\n",
            "epoch 37, train loss 0.1237, train acc 0.962, test loss 2.3680, test acc 0.538, time 0.0 sec\n",
            "epoch 38, train loss 0.1337, train acc 0.958, test loss 2.3857, test acc 0.560, time 0.0 sec\n",
            "epoch 39, train loss 0.1245, train acc 0.972, test loss 2.3685, test acc 0.527, time 0.0 sec\n",
            "epoch 40, train loss 0.1120, train acc 0.967, test loss 2.3571, test acc 0.560, time 0.0 sec\n",
            "epoch 41, train loss 0.1212, train acc 0.981, test loss 2.4131, test acc 0.571, time 0.0 sec\n",
            "epoch 42, train loss 0.0809, train acc 0.986, test loss 2.5477, test acc 0.505, time 0.0 sec\n",
            "epoch 43, train loss 0.0681, train acc 0.986, test loss 2.5616, test acc 0.527, time 0.0 sec\n",
            "epoch 44, train loss 0.0671, train acc 0.986, test loss 2.5744, test acc 0.538, time 0.0 sec\n",
            "epoch 45, train loss 0.0495, train acc 0.991, test loss 2.4908, test acc 0.560, time 0.0 sec\n",
            "epoch 46, train loss 0.0586, train acc 0.981, test loss 2.5480, test acc 0.538, time 0.0 sec\n",
            "epoch 47, train loss 0.0571, train acc 0.995, test loss 2.6429, test acc 0.538, time 0.0 sec\n",
            "epoch 48, train loss 0.0431, train acc 0.995, test loss 2.6626, test acc 0.538, time 0.0 sec\n",
            "epoch 49, train loss 0.0555, train acc 0.986, test loss 2.6689, test acc 0.538, time 0.0 sec\n",
            "epoch 50, train loss 0.0944, train acc 0.981, test loss 2.8446, test acc 0.505, time 0.0 sec\n",
            "epoch 51, train loss 0.0939, train acc 0.976, test loss 2.9510, test acc 0.516, time 0.0 sec\n",
            "epoch 52, train loss 0.0667, train acc 0.981, test loss 2.7996, test acc 0.516, time 0.0 sec\n",
            "epoch 53, train loss 0.1062, train acc 0.967, test loss 2.6789, test acc 0.538, time 0.0 sec\n",
            "epoch 54, train loss 0.1638, train acc 0.943, test loss 2.5358, test acc 0.549, time 0.0 sec\n",
            "epoch 55, train loss 0.1316, train acc 0.962, test loss 2.5739, test acc 0.538, time 0.0 sec\n",
            "epoch 56, train loss 0.1118, train acc 0.972, test loss 2.5926, test acc 0.571, time 0.0 sec\n",
            "epoch 57, train loss 0.0954, train acc 0.972, test loss 2.5658, test acc 0.560, time 0.0 sec\n",
            "epoch 58, train loss 0.0894, train acc 0.972, test loss 2.6238, test acc 0.582, time 0.0 sec\n",
            "epoch 59, train loss 0.0982, train acc 0.958, test loss 2.7151, test acc 0.516, time 0.0 sec\n",
            "epoch 60, train loss 0.0967, train acc 0.972, test loss 2.7646, test acc 0.516, time 0.0 sec\n",
            "epoch 61, train loss 0.1580, train acc 0.943, test loss 2.7310, test acc 0.549, time 0.0 sec\n",
            "epoch 62, train loss 0.1263, train acc 0.953, test loss 3.0442, test acc 0.527, time 0.0 sec\n",
            "epoch 63, train loss 0.1532, train acc 0.958, test loss 2.9884, test acc 0.473, time 0.0 sec\n",
            "epoch 64, train loss 0.0995, train acc 0.967, test loss 2.9604, test acc 0.495, time 0.0 sec\n",
            "epoch 65, train loss 0.1452, train acc 0.962, test loss 2.8687, test acc 0.527, time 0.0 sec\n",
            "epoch 66, train loss 0.1036, train acc 0.986, test loss 2.8780, test acc 0.538, time 0.0 sec\n",
            "epoch 67, train loss 0.0535, train acc 0.986, test loss 2.9268, test acc 0.549, time 0.0 sec\n",
            "epoch 68, train loss 0.0507, train acc 0.991, test loss 2.9961, test acc 0.527, time 0.0 sec\n",
            "epoch 69, train loss 0.0321, train acc 0.995, test loss 3.0145, test acc 0.516, time 0.0 sec\n",
            "epoch 70, train loss 0.0262, train acc 0.995, test loss 3.0390, test acc 0.505, time 0.0 sec\n",
            "epoch 71, train loss 0.0215, train acc 1.000, test loss 3.0655, test acc 0.516, time 0.0 sec\n",
            "epoch 72, train loss 0.0176, train acc 1.000, test loss 3.0452, test acc 0.516, time 0.0 sec\n",
            "epoch 73, train loss 0.0147, train acc 1.000, test loss 3.0538, test acc 0.516, time 0.0 sec\n",
            "epoch 74, train loss 0.0133, train acc 1.000, test loss 3.0883, test acc 0.516, time 0.0 sec\n",
            "epoch 75, train loss 0.0113, train acc 1.000, test loss 3.1219, test acc 0.516, time 0.0 sec\n",
            "epoch 76, train loss 0.0103, train acc 1.000, test loss 3.1475, test acc 0.516, time 0.0 sec\n",
            "epoch 77, train loss 0.0101, train acc 1.000, test loss 3.1669, test acc 0.516, time 0.0 sec\n",
            "epoch 78, train loss 0.0097, train acc 1.000, test loss 3.1877, test acc 0.516, time 0.0 sec\n",
            "epoch 79, train loss 0.0092, train acc 1.000, test loss 3.2031, test acc 0.516, time 0.0 sec\n",
            "epoch 80, train loss 0.0086, train acc 1.000, test loss 3.2168, test acc 0.516, time 0.0 sec\n",
            "epoch 81, train loss 0.0083, train acc 1.000, test loss 3.2320, test acc 0.516, time 0.0 sec\n",
            "epoch 82, train loss 0.0078, train acc 1.000, test loss 3.2478, test acc 0.516, time 0.0 sec\n",
            "epoch 83, train loss 0.0078, train acc 1.000, test loss 3.2618, test acc 0.516, time 0.0 sec\n",
            "epoch 84, train loss 0.0073, train acc 1.000, test loss 3.2760, test acc 0.516, time 0.0 sec\n",
            "epoch 85, train loss 0.0073, train acc 1.000, test loss 3.2867, test acc 0.516, time 0.0 sec\n",
            "epoch 86, train loss 0.0069, train acc 1.000, test loss 3.2970, test acc 0.516, time 0.0 sec\n",
            "epoch 87, train loss 0.0069, train acc 1.000, test loss 3.3057, test acc 0.516, time 0.0 sec\n",
            "epoch 88, train loss 0.0071, train acc 1.000, test loss 3.3164, test acc 0.516, time 0.0 sec\n",
            "epoch 89, train loss 0.0065, train acc 1.000, test loss 3.3248, test acc 0.516, time 0.0 sec\n",
            "epoch 90, train loss 0.0063, train acc 1.000, test loss 3.3363, test acc 0.516, time 0.0 sec\n",
            "epoch 91, train loss 0.0061, train acc 1.000, test loss 3.3433, test acc 0.516, time 0.0 sec\n",
            "epoch 92, train loss 0.0060, train acc 1.000, test loss 3.3507, test acc 0.516, time 0.0 sec\n",
            "epoch 93, train loss 0.0060, train acc 1.000, test loss 3.3584, test acc 0.516, time 0.0 sec\n",
            "epoch 94, train loss 0.0058, train acc 1.000, test loss 3.3642, test acc 0.516, time 0.0 sec\n",
            "epoch 95, train loss 0.0058, train acc 1.000, test loss 3.3710, test acc 0.516, time 0.0 sec\n",
            "epoch 96, train loss 0.0056, train acc 1.000, test loss 3.3760, test acc 0.516, time 0.0 sec\n",
            "epoch 97, train loss 0.0054, train acc 1.000, test loss 3.3821, test acc 0.516, time 0.0 sec\n",
            "epoch 98, train loss 0.0060, train acc 1.000, test loss 3.3887, test acc 0.516, time 0.0 sec\n",
            "epoch 99, train loss 0.0058, train acc 1.000, test loss 3.3942, test acc 0.516, time 0.0 sec\n",
            "epoch 100, train loss 0.0054, train acc 1.000, test loss 3.3982, test acc 0.516, time 0.0 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_accuracy(test_iter, model, loss = loss)"
      ],
      "metadata": {
        "id": "EY7aYEQ_rO0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6035239-abe9-4cfe-fad5-dae0a7939730"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5604395604395604, 2.3897581100463867)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDaRVNq1aMpm"
      },
      "source": [
        "# Problema 2\n",
        "\n",
        "Neste problema, você receberá 90 *features* extraídas de diversas músicas (datadas de 1922 até 2011) e deve predizer o ano de cada música. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWdBT3zhW_Y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea779ce8-8640-44a8-cf26-64664cbf8fa7"
      },
      "source": [
        "# download do dataset\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
        "!unzip YearPredictionMSD.txt.zip\n",
        "data = np.genfromtxt('YearPredictionMSD.txt', delimiter=',', dtype=np.float32)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-20 01:03:33--  http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘YearPredictionMSD.txt.zip’\n",
            "\n",
            "YearPredictionMSD.t     [            <=>     ] 201.24M  88.7MB/s    in 2.3s    \n",
            "\n",
            "2025-05-20 01:03:36 (88.7 MB/s) - ‘YearPredictionMSD.txt.zip’ saved [211011981]\n",
            "\n",
            "Archive:  YearPredictionMSD.txt.zip\n",
            "  inflating: YearPredictionMSD.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0, :])\n",
        "X, y = torch.from_numpy(data[:, 1:]),torch.from_numpy(data[:, 0])\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "X_train = torch.from_numpy(scaler.fit_transform(X_train)).to(device).float()\n",
        "X_test = torch.from_numpy(scaler.transform(X_test)).to(device).float()\n",
        "batch_size = 100\n",
        "train_iter = load_array(X_train, Y_train, batch_size)\n",
        "test_iter = load_array( X_test, Y_test, batch_size, False)"
      ],
      "metadata": {
        "id": "GUxNg0RhuERO",
        "outputId": "a00f9425-61bd-4406-949d-a8ecd8502b4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.0010000e+03  4.9943569e+01  2.1471140e+01  7.3077499e+01\n",
            "  8.7486095e+00 -1.7406281e+01 -1.3099050e+01 -2.5012020e+01\n",
            " -1.2232570e+01  7.8308902e+00 -2.4678299e+00  3.3213601e+00\n",
            " -2.3152101e+00  1.0205560e+01  6.1110913e+02  9.5108960e+02\n",
            "  6.9811426e+02  4.0898486e+02  3.8370911e+02  3.2651511e+02\n",
            "  2.3811327e+02  2.5142413e+02  1.8717351e+02  1.0042652e+02\n",
            "  1.7919498e+02 -8.4155798e+00 -3.1787039e+02  9.5862663e+01\n",
            "  4.8102589e+01 -9.5663033e+01 -1.8062149e+01  1.9698400e+00\n",
            "  3.4424381e+01  1.1726700e+01  1.3679000e+00  7.7944398e+00\n",
            " -3.6994001e-01 -1.3367851e+02 -8.3261650e+01 -3.7297649e+01\n",
            "  7.3046669e+01 -3.7366840e+01 -3.1385300e+00 -2.4215309e+01\n",
            " -1.3230660e+01  1.5938090e+01 -1.8604780e+01  8.2154793e+01\n",
            "  2.4057980e+02 -1.0294070e+01  3.1584311e+01 -2.5381870e+01\n",
            " -3.9077201e+00  1.3292580e+01  4.1550598e+01 -7.2627201e+00\n",
            " -2.1008631e+01  1.0550848e+02  6.4298561e+01  2.6084810e+01\n",
            " -4.4591099e+01 -8.3065701e+00  7.9370599e+00 -1.0736600e+01\n",
            " -9.5447662e+01 -8.2033073e+01 -3.5591942e+01  4.6952500e+00\n",
            "  7.0956261e+01  2.8091391e+01  6.0201502e+00 -3.7137669e+01\n",
            " -4.1124500e+01 -8.4081602e+00  7.1987700e+00 -8.6017599e+00\n",
            " -5.9085698e+00 -1.2324370e+01  1.4687340e+01 -5.4321251e+01\n",
            "  4.0147861e+01  1.3016200e+01 -5.4405479e+01  5.8993671e+01\n",
            "  1.5373440e+01  1.1114399e+00 -2.3087931e+01  6.8407951e+01\n",
            " -1.8222300e+00 -2.7463480e+01  2.2632699e+00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f6f8c2b9f8ba>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = torch.tensor(features)\n",
            "<ipython-input-4-f6f8c2b9f8ba>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keBjovYTsZGk"
      },
      "source": [
        "class NET2 (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NET2, self).__init__()\n",
        "\n",
        "    self.nn = nn.Sequential(\n",
        "\n",
        "      nn.Linear(90, 32),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(32, 16),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(16, 8),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(8, 4),\n",
        "      nn.ReLU(),\n",
        "\n",
        "\n",
        "\n",
        "      nn.Linear(4, 1),\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = self.nn(X)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = NET2().to(device)\n",
        "loss = nn.MSELoss()\n",
        "opt = optim.Adam(model2.parameters(), lr=0.001, weight_decay = 0.001)"
      ],
      "metadata": {
        "id": "e3LEshOStObf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_validate(net = model2, train_iter = train_iter , test_iter = test_iter, trainer = opt, loss = loss, num_epochs = 20, type='regression')"
      ],
      "metadata": {
        "id": "F6p-RgFuvkSP",
        "outputId": "9a7361cc-c1b3-475a-ca8a-e6e05667fe92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([81])) that is different to the input size (torch.Size([81, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, train loss 425580.0672, test loss 16416.9982, time 12.5 sec\n",
            "epoch 2, train loss 3452.1928, test loss 405.8616, time 12.3 sec\n",
            "epoch 3, train loss 231.7885, test loss 210.5230, time 12.3 sec\n",
            "epoch 4, train loss 163.6173, test loss 163.5506, time 13.4 sec\n",
            "epoch 5, train loss 156.6720, test loss 185.4185, time 12.4 sec\n",
            "epoch 6, train loss 151.8878, test loss 137.6621, time 12.3 sec\n",
            "epoch 7, train loss 147.3531, test loss 134.8163, time 12.3 sec\n",
            "epoch 8, train loss 168.9516, test loss 130.0984, time 12.3 sec\n",
            "epoch 9, train loss 143.4065, test loss 128.6453, time 12.3 sec\n",
            "epoch 10, train loss 154.1947, test loss 138.1776, time 12.2 sec\n",
            "epoch 11, train loss 143.6834, test loss 133.0499, time 12.2 sec\n",
            "epoch 12, train loss 149.8571, test loss 200.6762, time 12.6 sec\n",
            "epoch 13, train loss 151.6087, test loss 126.7141, time 12.3 sec\n",
            "epoch 14, train loss 144.7051, test loss 133.1477, time 12.3 sec\n",
            "epoch 15, train loss 146.6477, test loss 142.6437, time 12.2 sec\n",
            "epoch 16, train loss 147.9518, test loss 132.7487, time 12.3 sec\n",
            "epoch 17, train loss 146.7384, test loss 159.5635, time 12.3 sec\n",
            "epoch 18, train loss 145.8769, test loss 127.5185, time 12.4 sec\n",
            "epoch 19, train loss 149.7499, test loss 140.1077, time 12.3 sec\n",
            "epoch 20, train loss 142.2338, test loss 159.4692, time 12.9 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNMAyyX8jSb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed7bce0-32b2-40c9-d4d6-fcea3808ac0d"
      },
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\n",
        "y = model2(torch.Tensor(X_test[0:5, :]).to(device))\n",
        "print(y, Y_train[0:5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1993.2765],\n",
            "        [1994.4435],\n",
            "        [1995.2308],\n",
            "        [1997.6852],\n",
            "        [1995.0096]], device='cuda:0', grad_fn=<AddmmBackward0>) tensor([2005., 1994., 2007., 2005., 1990.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8hG7HCDUib"
      },
      "source": [
        "# Problema 3\n",
        "\n",
        "Neste problema, você receberá várias *features* (como altura média, inclinação, etc) descrevendo uma região e o modelo deve predizer qual o tipo da região (floresta, montanha, etc). Mais informações sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/covertype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZcIXGqBDznB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e682b3-1f1b-4874-d998-c350f6949981"
      },
      "source": [
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
        "!gzip covtype.data.gz\n",
        "data = np.genfromtxt('covtype.data', delimiter=',', dtype=np.float32)\n",
        "\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "train_labels = train_labels - 1\n",
        "test_labels = test_labels - 1\n",
        "\n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-20 01:21:37--  http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘covtype.data.gz’\n",
            "\n",
            "covtype.data.gz         [  <=>               ]  10.72M  25.9MB/s    in 0.4s    \n",
            "\n",
            "2025-05-20 01:21:38 (25.9 MB/s) - ‘covtype.data.gz’ saved [11240707]\n",
            "\n",
            "gzip: covtype.data.gz already has .gz suffix -- unchanged\n",
            "(581012, 55) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00]\n",
            "(581012, 54) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            "(581012,) 5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ-ERBKLSBBT",
        "outputId": "f76ce6a0-2faa-4da6-c4e7-db9cfbd4f257"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O0HJVIOZZW4"
      },
      "source": [
        "class NET3 (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NET3, self).__init__()\n",
        "\n",
        "    self.nn = nn.Sequential(\n",
        "\n",
        "      nn.Linear(54, 32),\n",
        "      nn.Dropout(0.5),\n",
        "\n",
        "      nn.Tanh(),\n",
        "\n",
        "\n",
        "      nn.Linear(32, 12),\n",
        "      nn.Dropout(0.3),\n",
        "      nn.Tanh(),\n",
        "\n",
        "      nn.Linear(12, 7),\n",
        "      nn.Dropout(0.15),\n",
        "      nn.Tanh(),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      nn.Linear(7, 7),\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = self.nn(X)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = NET3().to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model3.parameters(), lr=0.001, weight_decay = 0.001)"
      ],
      "metadata": {
        "id": "VdYuzcdhSuis"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_validate(net = model3, train_iter = train_iter , test_iter = test_iter, trainer = opt, loss = loss, num_epochs = 20, type='classification')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtPbwLFDSuQ3",
        "outputId": "1b966e97-1bbf-4b72-9217-a4f8ca3af7da"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 1.1998, train acc 0.478, test loss 1.1375, test acc 0.489, time 13.7 sec\n",
            "epoch 2, train loss 1.0721, train acc 0.508, test loss 0.9784, test acc 0.524, time 12.4 sec\n",
            "epoch 3, train loss 0.9416, train acc 0.590, test loss 0.9336, test acc 0.575, time 12.3 sec\n",
            "epoch 4, train loss 0.9309, train acc 0.598, test loss 0.9208, test acc 0.615, time 12.3 sec\n",
            "epoch 5, train loss 0.9250, train acc 0.601, test loss 0.9178, test acc 0.603, time 12.5 sec\n",
            "epoch 6, train loss 0.9235, train acc 0.601, test loss 0.9287, test acc 0.612, time 12.5 sec\n",
            "epoch 7, train loss 0.9194, train acc 0.605, test loss 0.9175, test acc 0.609, time 12.4 sec\n",
            "epoch 8, train loss 0.9223, train acc 0.604, test loss 0.9542, test acc 0.612, time 17.7 sec\n",
            "epoch 9, train loss 0.9194, train acc 0.603, test loss 0.9138, test acc 0.617, time 22.6 sec\n",
            "epoch 10, train loss 0.9184, train acc 0.609, test loss 0.9181, test acc 0.598, time 13.0 sec\n",
            "epoch 11, train loss 0.9130, train acc 0.610, test loss 0.9178, test acc 0.627, time 16.5 sec\n",
            "epoch 12, train loss 0.9090, train acc 0.613, test loss 0.9110, test acc 0.618, time 20.3 sec\n",
            "epoch 13, train loss 0.8920, train acc 0.630, test loss 0.8985, test acc 0.657, time 13.9 sec\n",
            "epoch 14, train loss 0.8831, train acc 0.637, test loss 0.8507, test acc 0.647, time 15.5 sec\n",
            "epoch 15, train loss 0.8765, train acc 0.640, test loss 0.8627, test acc 0.649, time 13.1 sec\n",
            "epoch 16, train loss 0.8719, train acc 0.642, test loss 0.9430, test acc 0.630, time 12.4 sec\n",
            "epoch 17, train loss 0.8648, train acc 0.645, test loss 0.8584, test acc 0.644, time 12.3 sec\n",
            "epoch 18, train loss 0.8628, train acc 0.644, test loss 0.8969, test acc 0.606, time 12.6 sec\n",
            "epoch 19, train loss 0.8643, train acc 0.644, test loss 0.8566, test acc 0.656, time 12.8 sec\n",
            "epoch 20, train loss 0.8570, train acc 0.646, test loss 0.9326, test acc 0.597, time 12.3 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOAxsRz0rpZl"
      },
      "source": [
        "# Problema 4\n",
        "\n",
        "Neste problema, você receberá 11 *features* extraídas de tipos de vinhos, e terá que predizer um *score* para cada vinho. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTzA0O6rusi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a61d68a-8c7f-48bd-e6c0-fb998ce82181"
      },
      "source": [
        "# download do dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
        "data_red = np.genfromtxt('winequality-red.csv', delimiter=';', dtype=np.float32, skip_header=1)\n",
        "data_white = np.genfromtxt('winequality-white.csv', delimiter=';', dtype=np.float32, skip_header=1)\n",
        "data = np.concatenate((data_red, data_white), axis=0)\n",
        "data = np.nan_to_num(data)\n",
        "\n",
        "print(data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, y.shape)\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-20 01:35:57--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘winequality-red.csv.3’\n",
            "\n",
            "winequality-red.csv     [ <=>                ]  82.23K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-05-20 01:35:57 (1.01 MB/s) - ‘winequality-red.csv.3’ saved [84199]\n",
            "\n",
            "--2025-05-20 01:35:57--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘winequality-white.csv.3’\n",
            "\n",
            "winequality-white.c     [ <=>                ] 258.23K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-20 01:35:57 (2.37 MB/s) - ‘winequality-white.csv.3’ saved [264426]\n",
            "\n",
            "[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
            "  0.56    9.4     5.    ]\n",
            "(6497, 11) (6497,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjoxzrL7T-uO"
      },
      "source": [
        "class NET4 (nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NET4, self).__init__()\n",
        "\n",
        "    self.nn = nn.Sequential(\n",
        "\n",
        "      nn.Linear(11, 4),\n",
        "      nn.Dropout(0.5),\n",
        "\n",
        "      nn.ReLU(),\n",
        "\n",
        "\n",
        "      nn.Linear(4, 12),\n",
        "      nn.Dropout(0.3),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      nn.Linear(12, 24),\n",
        "      nn.Dropout(0.15),\n",
        "      nn.ReLU(),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      nn.Linear(24, 1),\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = self.nn(X)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E7pwns4rx9l"
      },
      "source": [
        "model4 = NET4().to(device)\n",
        "loss = nn.MSELoss()\n",
        "opt = optim.Adam(model3.parameters(), lr=0.001, weight_decay = 0.001)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_validate(net = model4, train_iter = train_iter , test_iter = test_iter, trainer = opt, loss = loss, num_epochs = 50, type='regression')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT2RrhL1Uo3p",
        "outputId": "962fb89c-dc41-4e83-e158-0f05d61cce44"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 28.5391, test loss 28.5342, time 0.2 sec\n",
            "epoch 2, train loss 28.5459, test loss 28.5342, time 0.2 sec\n",
            "epoch 3, train loss 28.5306, test loss 28.5342, time 0.2 sec\n",
            "epoch 4, train loss 28.5182, test loss 28.5342, time 0.2 sec\n",
            "epoch 5, train loss 28.5558, test loss 28.5342, time 0.2 sec\n",
            "epoch 6, train loss 28.5343, test loss 28.5342, time 0.2 sec\n",
            "epoch 7, train loss 28.5754, test loss 28.5342, time 0.2 sec\n",
            "epoch 8, train loss 28.5218, test loss 28.5342, time 0.2 sec\n",
            "epoch 9, train loss 28.5623, test loss 28.5342, time 0.1 sec\n",
            "epoch 10, train loss 28.5697, test loss 28.5342, time 0.1 sec\n",
            "epoch 11, train loss 28.5251, test loss 28.5342, time 0.1 sec\n",
            "epoch 12, train loss 28.5536, test loss 28.5342, time 0.1 sec\n",
            "epoch 13, train loss 28.5474, test loss 28.5342, time 0.1 sec\n",
            "epoch 14, train loss 28.5377, test loss 28.5342, time 0.1 sec\n",
            "epoch 15, train loss 28.5559, test loss 28.5342, time 0.1 sec\n",
            "epoch 16, train loss 28.5839, test loss 28.5342, time 0.1 sec\n",
            "epoch 17, train loss 28.5570, test loss 28.5342, time 0.1 sec\n",
            "epoch 18, train loss 28.5299, test loss 28.5342, time 0.1 sec\n",
            "epoch 19, train loss 28.5442, test loss 28.5342, time 0.1 sec\n",
            "epoch 20, train loss 28.5480, test loss 28.5342, time 0.1 sec\n",
            "epoch 21, train loss 28.5528, test loss 28.5342, time 0.1 sec\n",
            "epoch 22, train loss 28.5543, test loss 28.5342, time 0.1 sec\n",
            "epoch 23, train loss 28.5466, test loss 28.5342, time 0.1 sec\n",
            "epoch 24, train loss 28.5722, test loss 28.5342, time 0.1 sec\n",
            "epoch 25, train loss 28.5351, test loss 28.5342, time 0.1 sec\n",
            "epoch 26, train loss 28.5368, test loss 28.5342, time 0.1 sec\n",
            "epoch 27, train loss 28.5455, test loss 28.5342, time 0.1 sec\n",
            "epoch 28, train loss 28.5277, test loss 28.5342, time 0.1 sec\n",
            "epoch 29, train loss 28.5558, test loss 28.5342, time 0.1 sec\n",
            "epoch 30, train loss 28.5270, test loss 28.5342, time 0.1 sec\n",
            "epoch 31, train loss 28.5534, test loss 28.5342, time 0.1 sec\n",
            "epoch 32, train loss 28.5515, test loss 28.5342, time 0.1 sec\n",
            "epoch 33, train loss 28.5329, test loss 28.5342, time 0.1 sec\n",
            "epoch 34, train loss 28.5387, test loss 28.5342, time 0.1 sec\n",
            "epoch 35, train loss 28.5273, test loss 28.5342, time 0.1 sec\n",
            "epoch 36, train loss 28.5094, test loss 28.5342, time 0.1 sec\n",
            "epoch 37, train loss 28.5335, test loss 28.5342, time 0.1 sec\n",
            "epoch 38, train loss 28.5413, test loss 28.5342, time 0.2 sec\n",
            "epoch 39, train loss 28.5514, test loss 28.5342, time 0.4 sec\n",
            "epoch 40, train loss 28.5446, test loss 28.5342, time 0.2 sec\n",
            "epoch 41, train loss 28.5569, test loss 28.5342, time 0.1 sec\n",
            "epoch 42, train loss 28.5585, test loss 28.5342, time 0.1 sec\n",
            "epoch 43, train loss 28.5775, test loss 28.5342, time 0.1 sec\n",
            "epoch 44, train loss 28.5459, test loss 28.5342, time 0.1 sec\n",
            "epoch 45, train loss 28.5577, test loss 28.5342, time 0.1 sec\n",
            "epoch 46, train loss 28.5437, test loss 28.5342, time 0.1 sec\n",
            "epoch 47, train loss 28.5569, test loss 28.5342, time 0.1 sec\n",
            "epoch 48, train loss 28.5628, test loss 28.5342, time 0.1 sec\n",
            "epoch 49, train loss 28.5188, test loss 28.5342, time 0.1 sec\n",
            "epoch 50, train loss 28.5418, test loss 28.5342, time 0.1 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-At99Iqzryg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3408c667-3684-41b0-dfba-d7963f308385"
      },
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\n",
        "y = model4(torch.Tensor(test_features[0:5, :]).to(device))\n",
        "print(y, test_labels[0:5])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6065],\n",
            "        [0.5670],\n",
            "        [0.6725],\n",
            "        [0.9656],\n",
            "        [1.0701]], device='cuda:0', grad_fn=<AddmmBackward0>) [8. 5. 7. 6. 6.]\n"
          ]
        }
      ]
    }
  ]
}